---
title: 源码解读-HashMap
date: 2018-03-14
tags:
- Java
---
<details>
<summary>点击展开目录</summary>
<!-- TOC -->

- [关于Hash](#关于hash)
- [结构](#结构)
- [Hash方法](#hash方法)
- [例](#例)
- [线程安全](#线程安全)
    - [Java7](#java7)
    - [Java8](#java8)
    - [线程安全的Map](#线程安全的map)
- [fail-fast](#fail-fast)
- [序列化问题](#序列化问题)
- [方法](#方法)
- [Usage](#usage)
- [Q&A](#qa)

<!-- /TOC -->
</details>

# 关于Hash

通常来说Hash算法具有以下特征,

* 输入和哈希值是多对一的关系,即是一种压缩映射.最优情况是一对一
* 哈希值不同,输入肯定不同
* 哈希值相同,输入可能不同
* 同样的输入,哈希值必须相同
* 不同的输入,同样的哈希值,这种现象被称为哈希碰撞,碰撞几率是衡量一个哈希算法优劣的重要指标.

解决碰撞的方案:

* 开放定址法:一旦发生了冲突, 就去寻找下一个空的散列地址, 只要散列表足够大, 空的散列地址总能找到, 并将记录存入
* 链地址法:将哈希表的每个单元作为链表的头结点, 所有哈希地址为i的元素构成一个同义词链表即发生冲突时就把该关键字链在以该单元为头结点的链表的尾部
* 再哈希法:当哈希地址发生冲突用其他的函数计算另一个哈希函数地址, 直到冲突不再产生为止
* 建立公共溢出区:将哈希表分为基本表和溢出表两部分, 发生冲突的元素都放入溢出表中

HashMap即是采用链地址法,结合数组和链表两者的优势.

# 结构

```Java
// 默认容量
static final int DEFAULT_INITIAL_CAPACITY = 1 << 4;
// 默认扩容因子
static final float DEFAULT_LOAD_FACTOR = 0.75f;
// 链表重构为红黑树的控制因子
static final int TREEIFY_THRESHOLD = 8;
// 扩容临界值
int threshold;
// 节点类
static class Node<K,V> implements Map.Entry<K,V> {
    final int hash;// key的hash值
    final K key;
    V value;
    Node<K,V> next;// key的hash值相同时使用
}
transient Node<K,V>[] table;
transient Set<Map.Entry<K,V>> entrySet;
transient int size;
// 红黑树节点类,向上继承自上面的节点类
static final class TreeNode<K,V> extends LinkedHashMap.Entry<K,V> {
    TreeNode<K,V> parent;  // red-black tree links
    TreeNode<K,V> left;
    TreeNode<K,V> right;
    TreeNode<K,V> prev;    // needed to unlink next upon deletion
    boolean red;
    TreeNode(int hash, K key, V val, Node<K,V> next) {
        super(hash, key, val, next);
    }
}
```

* 默认初始化大小为16,默认使用75%的容量后进行扩容,变为原来的2倍(位运算左移1位)
* 遍历和插入顺序不同
* 存储容器为数组,即代码中的属性table,存储的是节点对象
* key可以为null,重复则会覆盖现有的(默认),也可以使用`putIfAbsent()`方法不覆盖.value也可以为空
* 非线程安全
* 扩容后链表的顺序不再和Java7一样,不会反向
* 存在同一个槽位的key,其hash值可能并不相同,因为还和`(capacity-1)`进行了与运算

底层:数组+链表(红黑树),初始化是可以自定义初始化大小和扩容因子(有参构造),使用静态内部类对象作为节点,重构后节点类型变为树节点类型,两者存在继承关系.


# Hash方法

```Java
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
// 计算下标
// (n - 1) & hash
```
上面是HashMap的hash算法,及获取下标的计算式,那么就有两个问题:
为什么要自定义一个hash方法?
为什么用hash值与数组长度-1的与操作的结果作为数组的下标?


以默认大小16为例:

| 方法                | 结果                                      | 十进制 |
| :------------------ | :---------------------------------------- | -----: |
| `h=hashcode()`      | `0000 0000 0000 0001 1001 1110 0101 1111` | 106079 |
| `h >>> 16`          | `0000 0000 0000 0000 0000 0000 0000 0001` |      1 |
| `hash=h^(h >>> 16)` | `0000 0000 0000 0001 1001 1110 0101 1110` | 106078 |
| `n-1`               | `0000 0000 0000 0000 0000 0000 0000 1111` |     15 |
| `(n-1)&hash`        | `0000 0000 0000 0000 0000 0000 0000 1110` |     14 |

任何数和0异或都是其本身,因此h高16位不变,而数组的大小也是2的幂运算(2的k次方,k>=4),
因此下标由长度的低n位主导,这样会增大hash碰撞的几率,因此在与操作前先进行移位,异或操作使高16位能参与运算以降低碰撞几率.


# 例

以下重点分析大量hash碰撞发生时,链表向红黑树重构的过程:

添加用方法:

put putVal
```Java
final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                boolean evict) {
    Node<K,V>[] tab; Node<K,V> p; int n, i;
    if ((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
    if ((p = tab[i = (n - 1) & hash]) == null)// 数组的对应位置没有元素就加进去
        tab[i] = newNode(hash, key, value, null);
    else {// 加进去的地方已经有值,可以简单的认为发生hash碰撞,其实hash值可能并不相同
        Node<K,V> e; K k;
        if (p.hash == hash &&
            ((k = p.key) == key || (key != null && key.equals(k))))// 键允许为空体现在这里,也考虑到使用相同的键添加元素情况
            e = p;
        else if (p instanceof TreeNode)// 红黑树节点,往树上添加节点
            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        else {// 即不是空键也不是相同的键也没有重构成红黑树时
            for (int binCount = 0; ; ++binCount) {
                if ((e = p.next) == null) {
                    p.next = newNode(hash, key, value, null);// 继续往链表上加
                    if (binCount >= TREEIFY_THRESHOLD - 1) // >=7,即超过8个节点时(=7时,原有8个,上一步加一个,共有9个节点)重构
                        treeifyBin(tab, hash);// 重构操作,在内部将所有节点的类型转为树节点类型(原来的子类)
                    break;
                }
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                p = e;
            }
        }
        if (e != null) { // existing mapping for key
            V oldValue = e.value;
            if (!onlyIfAbsent || oldValue == null)
                e.value = value;
            afterNodeAccess(e);// LinkedHashMap使用
            return oldValue;
        }
    }
    ++modCount;
    if (++size > threshold)// 扩容校验,超过容量的75%,容量的确定,倍增等都在此方法中实现
        resize();// 具体进行扩容的方法
    afterNodeInsertion(evict);// LinkedHashMap使用
    return null;
}
```

当数组需要扩容时,是调用`resize()`方法,并将旧数组的数组拷贝到新数组中

查找用的方法:

```Java
final Node<K,V> getNode(int hash, Object key) {
    Node<K,V>[] tab; Node<K,V> first, e; int n; K k;
    if ((tab = table) != null && (n = tab.length) > 0 &&
        (first = tab[(n - 1) & hash]) != null) { // 计算出应该存储在数组的哪里,并获取到链上第一个节点
        if (first.hash == hash &&
            ((k = first.key) == key || (key != null && key.equals(k)))) // 判断第一个节点是否是get的对象
            return first;
        if ((e = first.next) != null) {// 不是第一个节点
            if (first instanceof TreeNode)// 此处TreeNode为树对象,满足则走树查找算法
                return ((TreeNode<K,V>)first).getTreeNode(hash, key);
            do { // 链表没有没有重构为红黑树,遍历整个链表
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    return e;
            } while ((e = e.next) != null);
        }
    }
    return null;
}
```

删除时也是类似,先判断数组中的是否是要查找的,不是则判断是链式结构还是树形结构进行查找,删除.

# 线程安全

上面说到HashMap是非线程安全的,那么在并发环境下使用HashMap会有什么后果呢?

## Java7

在Java7中,扩容完成后会执行下面的代码,其作用是将旧table的数据移到扩容后的新table中去:
[源码地址](https://github.com/lambdalab-mirror/jdk7u-jdk/blob/master/src/share/classes/java/util/HashMap.java#L589)
```Java
void transfer(Entry[] newTable, boolean rehash) {
    int newCapacity = newTable.length;
    for (Entry<K,V> e : table) {
        while(null != e) {// ②
            Entry<K,V> next = e.next;// ①
            if (rehash) {
                e.hash = null == e.key ? 0 : hash(e.key);
            }
            // 槽位index, 链式结构反转的前提
            int i = indexFor(e.hash, newCapacity);
            e.next = newTable[i];// ③
            newTable[i] = e;// ④
            e = next;
        }
    }
}
```

可以看出来,如果其中的链式节点经过rehash后仍然在同一槽位(即`int i`的i相同),那么迁移后这2个节点顺序就会反向(由最后3行代码实现翻转).
假设有这样一个并发环境, 目前某槽位链中发生以下过程:

1. 线程1执行到①处, 同时线程2执行到②处, 此时线程1和2的变量`e`为同一个对象, 此时满足`e->next`的链式关系
2. 当线程1执行到④处结束后, 执行`newTable[i] = e`, 此时`e`节点成为新数组中对应槽位的首节点
3. 线程2开始执行③语句, 获取到新数组槽位的首节点, 设置了`e.next=newTable[i]`, 就会出现`e`指向的这个对象`e.next=e`
3. 最终形成环状结构, 出现死循环, 即使这个死循环能够跳出来, 那么查找这个链表上的数据时也会出现死循环.

这种情况下可以使用使用jdk的命令确定错误的发生场所:`jps;jstack <pid>`,

值得注意的是,虽然是非线程安全的,但并不是在并发下使用就一定会出现这种情况.

## Java8

Java8中不再使用上面的结构,扩容和数据迁移都是在`resize()`方法中实现,
[源码地址](https://github.com/lambdalab-mirror/jdk8u-jdk/blob/master/src/share/classes/java/util/HashMap.java#L714)
```Java
Node<K,V> loHead = null, loTail = null;// 原下标位置的链表
Node<K,V> hiHead = null, hiTail = null;// 新位置的链表
Node<K,V> next;
do {
    next = e.next;
    if ((e.hash & oldCap) == 0) { // 判断扩容前后槽位是否变化
        if (loTail == null)
            loHead = e;
        else
            loTail.next = e;
        loTail = e;
    }
    else { // 槽位发生变化的key
        if (hiTail == null)
            hiHead = e;
        else
            hiTail.next = e;
        hiTail = e;
    }
} while ((e = next) != null);
if (loTail != null) {
    loTail.next = null;
    newTab[j] = loHead;
}
if (hiTail != null) {
    hiTail.next = null;
    newTab[j + oldCap] = hiHead;
}
```

上面这一段代码发生在扩容完成后,数据迁移时某个槽位内容为链表(内容为单元素,红黑树以外)时,可以看到是将一个链表分成了2个链表,新位置的确定也不再使用rehash,不会再出现环状结构.

另外,我们知道在同一个槽位的hash值不一定相同,那么什么样的key被放入哪个链表中呢?
下面以容量16扩容到32为例:

扩容前:

| 最大下标/key | 二进制                                    |
| :----------- | :---------------------------------------- |
| `n-1(15)`    | `0000 0000 0000 0000 0000 0000 0000 1111` |
| `key1`       | `1111 1111 1111 1111 0000 1111 0000 0101` |
| `key1^(n-1)` | `0000 0000 0000 0000 0000 0000 0000 0101` |
| `key2`       | `1111 1111 1111 1111 0000 1111 0001 0101` |
| `key2^(n-1)` | `0000 0000 0000 0000 0000 0000 0000 0101` |

扩容后:

| 最大下标/key | 二进制                                    |
| :----------- | :---------------------------------------- |
| `n-1(31)`    | `0000 0000 0000 0000 0000 0000 0001 1111` |
| `key1`       | `1111 1111 1111 1111 0000 1111 0000 0101` |
| `key1^(n-1)` | `0000 0000 0000 0000 0000 0000 0000 0101` |
| `key2`       | `1111 1111 1111 1111 0000 1111 0001 0101` |
| `key2^(n-1)` | `0000 0000 0000 0000 0000 0000 0001 0101` |

通过或运算计算槽位,但在扩容前key1和key2在同一个槽位,但扩容后就不在一个槽位了.
上面代码中有个条件`(e.hash & oldCap) == 0`,满足这个条件的处在原槽位不动,不满足的则移动到`原槽位+oldCap`的槽位.

但为什么满足这个条件的就是扩容前后槽位计算结果不会发生变化的呢?
因为容量是2的指数幂结果,二进制表现就是只有`指数+1`位为1,其他为0,
既然结果为0,那么hashcode的该位肯定为0,扩容前后,该位的与运算的结果都是0,即使扩容将`容量-1`的该位从0变为了1.

对于那些槽位会发生变化的key来说,其变化正好是增加了半个新容量(整个旧容量)的大小.

虽然Java8已经不会引起死循环但数据丢失的问题仍然存在,因此还是不推荐在并发环境下使用HashMap.


## 线程安全的Map

```Java
// 0. jdk提供
Map<String, String> map = new ConcurrentHashMap<>(8);
// 1. 工具类
Map<String, String> map = Collections.synchronizedMap(new HashMap<>(8));
// 2. Guava
Map<String, String> map = Maps.newConcurrentMap();
```
> HashTable也是安全的, 但使用`synchronized`来保证的, put,get都会加锁, 高并发下性能低下

# fail-fast

HashMap的迭代器(Iterator)是`fail-fast`迭代器, 而Hashtable的enumerator迭代器不是`fail-fast`的.

所以当有其它线程改变了HashMap的结构（增减元素时）, 将会抛出`ConcurrentModificationException`,

但迭代器本身的remove()方法移除元素则不会抛出`ConcurrentModificationException`异常.

值得注意的是这并不是一定发生的行为,它只是一种错误检测机制,不可能对是否出现并发修改作出任何保证.这条同样也是`Enumeration`和`Iterator`的区别

# 序列化问题

阅读源码知道,HashMaphe和ArrayList一样都实现了Serializable接口,
但最终的存储容器table确实被transient修饰的,也就是说这个属性在序列化的时候被忽略,
原因我们知道是因为当一个key是自定义对象时,如果没有正确重写hashCode()方法的话,在不同环境下的hash值可能不同,这也就是推荐使用String等对象做key的原因.
另外就是table中有很多null的,这些null是没有必要序列化的,

序列化一个类的对象时,如果这个类实现了writeObject和readObject方法,就会使用这两个方法进行序列化和反序列化,

没实现就使用`defaultWriteObject()`和`defaultReadObject()`方法

而HashMap实现了这两个方法:
```Java
private void writeObject(java.io.ObjectOutputStream s)
private void readObject(java.io.ObjectInputStream s)
```

# 方法


| 方法                                                  | 效果               |
| --------------------------------------------------- | ---------------- |
| `public V put(K key, V value)`                      | 不存在则插入, 存在替换值    |
| `public V putIfAbsent(K key, V value)`              | 不存在插入, 存在不操作     |
| merge                                               | 和put效果相同         |
| compute                                             | 和put效果相同         |
| computeIfAbsent                                     | 不存在插入, 存在不操作     |
| computeIfPresent                                    | 存在替换值, 不存在不操作    |
| `public V getOrDefault(Object key, V defaultValue)` | 不存在key时获取到设置的默认值 |

**putIfAbsent()**

向map中添加k-v时先判断其是否存在, 不存在则会进行添加并返回null, 如果存在则不会继续操作直接返回已经存在的值.

在HashMap和ConcurrentHashMap中虽然对此方法有重写, 但仍保持着不覆盖现有的做法.

**computeIfAbsent()**

`public V computeIfAbsent(K key, Function<? super K, ? extends V> mappingFunction)`

当map对象调用此方法时, 会先判断是否有指定key的k-v, 没有则会使用`mappingFunction`计算出value(计算过程传入参数`key`), 然后将k-v存入map, 有则会直接读取

**computeIfPresent**

`public V computeIfPresent(K key, BiFunction<? super K, ? super V, ? extends V> remappingFunction)`

调用此方法, 会判断是否存在指定的key, 不存在则退出, 存在则会使用 `remappingFunction` 计算出value(传入的参数是`key`和存在的`oldValue`)替换掉现有的value

**compute**

`public V compute(K key, BiFunction<? super K, ? super V, ? extends V> remappingFunction)`

`computeIfAbsent/computeIfPresent`的综合体, key存在与不存在都可以使用此方法计算出

重映射参数的方法则是`key`和`oldValue`(可能为null)

**merge**

`public V merge(K key, V value, BiFunction<? super V, ? super V, ? extends V> remappingFunction)`

不存在则`put(key, value)`, 存在则会使用oldValue和给的value传参进入重映射函数中进行计算出新的value, 替换掉现有的value

# Usage

遍历HashMap的好方式:

```Java
for (Map.Entry<String, String> entry : map.entrySet()) {
    System.out.println(entry.getKey() + ":" + entry.getValue());
}

Iterator<Map.Entry<String, String>> entryIterator = map.entrySet().iterator();
while (entryIterator.hasNext()) {
    Map.Entry<String, String> a = entryIterator.next();
}

for (String key : map.keySet()) {
    System.out.println(key + ":" + map.get(key));
}

Iterator keyIterator = map.keySet().iterator();
while (keyIterator.hasNext()) {
    String key = keyIterator.next();
    System.out.println(key + ":" + map.get(key));
}

map.forEach((key, value) -> System.out.println(key + ":" + value));
```

> 同时需要k-v时, 使用entrySet方式
> 只需要k时不需v, 使用keySet

**排序**

如统计一组文章单词的出现次数, 并排序

```Java
Map<String, Integer> map = Maps.newHashMap();
map.put("", map.getOrDefault("", 0) + 1);
List<String> collect = map.entrySet().stream()
        .sorted(
                // Comparator.comparing(e -> e.getValue())
                // Comparator.comparing(Map.Entry::getValue)
                Map.Entry.comparingByValue()
        )
        .map(e -> e.getKey()).collect(Collectors.toList());
```

# Q&A

***键***

要计算hashCode(),就要防止键改变,存取时键的值不同,其hashcode会不同,这样是不能正确取出值的.

使用不可变的, 声明作final的对象,并且采用合适的equals()和hashCode()方法的话,将会减少碰撞的发生,提高效率.

不可变性使得能够缓存不同键的hashcode,这将提高整个获取对象的速度,因此推荐使用String,Interger这样的wrapper类

因为包装类的一个对象一经创建,其所代表的值将不再变化,也就是说不能通过改变其引用来改变它的值,直至它被垃圾回收器回收

**Hashmap的容量为什么是2的幂次**

在计算下标时, 会使用到 `h & (len - 1) == h % len` 公式

当容量一定是`2^n`时, 这个公式中的`len - 1`在计算时候, 二进制表现全部为1, 在进行与操作的时候, 计算出的下标取决于Key的Hashcode值的最后几位

也就是说, key的后几位, 每一位都能够对计算的结果产生影响, 从而降低hash算法碰撞率, 假设其中的某个1变成了0, 那个key的对应位就无法起作用

其次, 位运算的效率是远远高于取余`%`运算的(并不是绝对), 而2的n次幂是很好的支持位运算的

**初始大小为什么是16**

16应该是个经验值, 不能太大也不能太小, 大了使用不了浪费空间, 小了频繁扩容, rehash降低性能

实际上, 在创建时推荐使用公式 `expectedSize / 0.75 + 1` 计算初始值, 为什么是这个公式呢?

举个栗子, 加入要存储7个元素,于是传入7, 实际内部使用的是8, 而在存储6(`8*0.75`)个的时候就会发生扩容

而使用公式计算出设置为10(`7*4/3 + 1`), 实际使用的就是16, 不会在存储了6个后发生扩容

这个公式可以阅读`putAll()`方法部分的源码

另外, Guava中提供了方法:

`Map<String, String> map = Maps.newHashMapWithExpectedSize(10);`

这个方法就使用了这个公式, 只需要传入预计存储的数量即可

**HashMap自己写了一个hash方法来计算hash值, 为什么不用key本身的hashCode方法, 而是又处理了一下?**


**`Map.Entry  TreeNode` 的关系**

HashMap.TreeNode -> LinkedHashMap.Entry -> HashMap.Node -> Map.Entry


